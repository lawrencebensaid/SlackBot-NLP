{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00000-c2012faa-01ff-4886-a8bd-949126d80829",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "711e5d22",
    "execution_start": 1624998994153,
    "execution_millis": 3307,
    "deepnote_cell_type": "code"
   },
   "source": [
    "! pip install nltk\n",
    "! pip install numpy\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "! python -m nltk.downloader all\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "print('Setup done!')"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "zsh:1: command not found: pip\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.21.0-cp39-cp39-macosx_10_9_x86_64.whl (16.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 16.9 MB 8.6 MB/s \n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-1.21.0\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2abf4d2dec7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' python -m nltk.downloader all'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "intents = json.loads(open('./input/intents.json').read())\n",
    "\n",
    "words = []\n",
    "topics = []\n",
    "documents = []\n",
    "ignore_letters = ['?', '!', '.', ',', ':', ';', '\"', '\\'']\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        word_list = nltk.word_tokenize(pattern)\n",
    "        for word in word_list:\n",
    "            words.append(word)\n",
    "        documents.append((word_list, intent['topic']))\n",
    "        if intent['topic'] not in topics:\n",
    "            topics.append(intent['topic'])\n",
    "\n",
    "words = [lemmatizer.lemmatize(word, 'v') for word in words if word not in ignore_letters]\n",
    "words = sorted(set(words))\n",
    "topics = sorted(set(topics))\n",
    "\n",
    "pickle.dump(words, open('output/words.pk1', 'wb'))\n",
    "pickle.dump(topics, open('output/topics.pk1', 'wb'))\n",
    "\n",
    "training = []\n",
    "output_empty = [0] * len(topics)\n",
    "\n",
    "for document in documents:\n",
    "    bag = []\n",
    "    word_patterns = document[0]\n",
    "    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
    "    for word in words:\n",
    "        bag.append(1 if word in word_patterns else 0)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[topics.index(document[1])] = 1\n",
    "    training.append([bag, output_row])"
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00001-acd13767-79ad-406e-9d12-dc3f7b3a6c94",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "8e757c1f",
    "execution_start": 1624999101193,
    "execution_millis": 10,
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'WordNetLemmatizer' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3b7fe32e82c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mintents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./input/intents.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordNetLemmatizer' is not defined"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "training"
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00002-55c52d4f-6962-4386-b93a-df4855b9a121",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "95a51d89",
    "execution_start": 1624999095717,
    "execution_millis": 9,
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'training' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-81c24f409a8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'training' is not defined"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "train_x = list(training[:, 0])\n",
    "train_y = list(training[:, 1])"
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00003-87e2de4c-2c75-427c-95bb-d79eb55fee42",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "9ba85c37",
    "execution_start": 1624999081798,
    "execution_millis": 5,
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'training' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-0bca55c3462d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training' is not defined"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "model = Sequential([\n",
    "    Dense(128, input_shape=(len(train_x[0]),), activation='relu'),\n",
    "    Dropout(.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(.5),\n",
    "    Dense(len(train_y[0]), activation='softmax'),\n",
    "])\n",
    "\n",
    "sgd = SGD(lr=.01, decay=1e-6, momentum=.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00003-62909a02-5e41-4607-872b-f84afcf728f5",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "64466ce3",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
    "model.save('output/SlackBotNLP.model')\n",
    "print(\"All set!\")"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "ca4a1788-90f9-46aa-a640-e6563821b647",
  "deepnote_execution_queue": [],
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('3.7.10')"
  },
  "interpreter": {
   "hash": "91f696de64eae175f120101ac49710d0de48d6ddb2377cb34b98bab1f5e43f26"
  }
 }
}